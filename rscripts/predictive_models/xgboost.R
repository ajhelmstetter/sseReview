

rm(list = ls())
library(xgboost)

#read in data
df <- read.csv("data/sse_review_table - main_table.csv")

#check col names
colnames(df)

#choose the columns you want in model (plus study/model_no for formatting)
df <-
  df[, c(
    'study',
    'model_no',
    'order',
    'trait_type_1',
    'sse_model',
    'tips',
    'year',
    'no_markers',
    'age',
    'age_inferred',
    'div_inc',
    'no_plastid',
    'no_mito',
    'no_nuclear',
    'perc_sampling',
    'samples_per_state'
  )]


###
# Data transformations
###

#log no_markers as long tail
hist(df$no_markers)
df$no_markers <- log(df$no_markers)
hist(df$no_markers)

#log tips as long tail
hist(df$tips)
df$tips <- log(df$tips)
hist(df$tips)

#log age as long tail
#still strange
hist(df$age)
df$age <- log(df$age)
hist(df$age)


hist(df$perc_sampling)

#arcsine
df$perc_sampling <- asin(sqrt(df$perc_sampling))
hist(df$perc_sampling)

#change to numeric prior to transformation, remove NAs
df$samples_per_state <- as.numeric(df$samples_per_state)

#fix one study (Sabath et al.) with 0
df$samples_per_state[df$samples_per_state == 0] <- NA

#log transform
hist(df$samples_per_state)
df$samples_per_state <- log(df$samples_per_state)
hist(df$samples_per_state)

df$samples_per_state[is.na(df$samples_per_state)] <- 0

###
# Make samples per state into tip bias
###

#remove quasse for div_inc
#may need to remove others too
df <- df[df$sse_model != "QuaSSE", ]

#make sure factors are factors
df$order <- as.factor(df$order)
df$trait_type <- as.factor(df$trait_type)
df$sse_model <- as.factor(df$sse_model)
df$year <- as.factor(df$year)



#make samples per state into tip bias
df$samples_per_state

#make column with combination of study and model
tmp_df <- df %>% tidyr::unite("study_model", 1:2, remove = T)

#reduce dataset to two columns
tmp_df <- tmp_df[, c("study_model", "samples_per_state")]

head(tmp_df)

#get max and min values of samples per state for each model
top_df <-
  tmp_df %>% group_by(study_model) %>% slice_max(n = 1,
                                                 order_by = samples_per_state,
                                                 with_ties = F)
bot_df <-
  tmp_df %>% group_by(study_model) %>% slice_min(n = 1,
                                                 order_by = samples_per_state,
                                                 with_ties = F)

####
# Check correlation of continuous vars
####

df_cor <-
  df[, c(
    'tips',
    'no_markers',
    'age',
    'no_plastid',
    'no_mito',
    'no_nuclear',
    'perc_sampling',
    'samples_per_state'
  )]

cor(df_cor)

#reduce to a single binary result per model per study
df3 <- df %>%
  group_by(study, model_no) %>%
  dplyr::slice(which.max(div_inc))

#make binary trait factor
df3$div_inc <- as.factor(df3$div_inc)

#make sampling fraction %
df3$perc_sampling <- df3$perc_sampling * 100

#add tip bias column by dividing larger number of tips with state A by smaller number of tips with state B
#multi-state models are therefore largest tip bias possible in the data
df3$tip_bias <- top_df$samples_per_state / bot_df$samples_per_state

#convert back to df
df <- as.data.frame(df3)

#remove study and model no (first two rows)
df <- df[, -which(colnames(df)%in%c('study',
                                    'model_no',
                                    'samples_per_state'))]

library(data.table)
df <- data.table(df, keep.rownames = FALSE)
df[is.na(df)] <- 0
head(df)
str(df)

#can include ordinal vars
#ordinal variable : can take a limited number of values (like factor) ;
#these values are ordered (unlike factor). Here these ordered values are: Marked > Some > None
#GOOD FOR MUSSE?

#One-hot encoding
#Next step, we will transform the categorical data to dummy variables. This is the one-hot encoding step.

#The purpose is to transform each value of each categorical feature in a binary feature {0, 1}.
#Response is excluded because it will be our label column, the one we want to predict.
require(Matrix)
require(xgboost)
sparse_matrix <- sparse.model.matrix(div_inc ~ . - 1, data = df)
head(sparse_matrix)

#Formula div_inc~.-1 used above means transform all categorical features but column div_inc to binary values.
#The -1 is here to remove the first column which is full of 1 (this column is generated by the conversion).

output_vector = df[, div_inc] == 1

#Build the model
bst <-
  xgboost(
    data = sparse_matrix,
    label = as.numeric(output_vector),
    max.depth = 4,
    eta = 1,
    nthread = 1,
    nrounds = 2,
    objective = "binary:logistic"
  )

importance <-
  xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst)
head(importance)

#Gain is the improvement in accuracy brought by a feature to the branches it is on.
#The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements,
#after adding the split on this feature, there are two new branches, and each of these branch is more accurate
#(one branch saying if your observation is on this branch then it should be classified as 1,
#and the other branch saying the exact opposite).

#Cover measures the relative quantity of observations concerned by a feature.

#Frequency is a simpler way to measure the Gain.
#It just counts the number of times a feature is used in all generated trees.
#You should not use it (unless you know why you want to use it).

importanceRaw <-
  xgb.importance(
    feature_names = sparse_matrix@Dimnames[[2]],
    model = bst,
    data = sparse_matrix,
    label = output_vector
  )

importanceRaw

#Plotting the feature importance
xgb.ggplot.importance(importance_matrix = importanceRaw)

ggsave("figures/xgboost_importance.pdf")

