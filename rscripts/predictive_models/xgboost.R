####
# Predicting SSE model results with xgboost
####

rm(list = ls())

library(xgboost)
library(tidyr)
library(dplyr)
library(ggplot2)
library(Matrix)
library(caret)
library(mlr)

#read in data
df <- read.csv("data/sse_review_table - main_table.csv")

#check col names
colnames(df)

#choose the columns you want in model (plus study/model_no for formatting)
df <-
  df[, c(
    'study',
    'model_no',
    'year',
    'order',
#    'family',
    'level',
#    'clade',
    'trait_level_1',
    'trait_level_2',
    'trait_level_3',
    'trait_level_4',
#    'trait_level_5',
#    'trait_level_6',
    'sse_model',
    'tips',
    'age',
    'age_inferred',
    'no_markers',
#    'no_plastid',
#    'no_mito',
#    'no_nuclear',
    'perc_sampling',
#    'sampling_per_state' #varies among states
    'samples_per_state',
#    'putative_ancestral_state', #varies among states
    'div_inc'
  )]


###
# Data preparation
###

###
# Make samples per state into tip bias
###

#get max and min values of samples per state for each model
top_df <-
  df %>% group_by(study, model_no) %>% slice_max(n = 1,
                                                 order_by = samples_per_state,
                                                 with_ties = F)

head(top_df)

bot_df <-
  df %>% group_by(study, model_no) %>% slice_min(n = 1,
                                                 order_by = samples_per_state,
                                                 with_ties = F)

head(bot_df)

#reduce to a single binary result per model per study
df3 <- df %>%
  group_by(study, model_no) %>%
  dplyr::slice(which.max(div_inc))

#set Multi-State results to 1
df3$div_inc[df3$div_inc > 1] <- 1

#make binary trait factor
df3$div_inc <- as.factor(df3$div_inc)

#add tip bias column by dividing larger number of tips with state A by smaller number of tips with state B
#multi-state models are therefore largest tip bias possible in the data
df3$tip_bias <- top_df$samples_per_state / bot_df$samples_per_state

#check that orders of df2 and top/bot_df match up
top_df$study==bot_df$study
top_df$model_no==bot_df$model_no

df3$model_no==bot_df$model_no
df3$model_no==top_df$model_no

df3$study==bot_df$study
df3$study==top_df$study

#get rid of inf tip bias value
df3$tip_bias[is.infinite(df3$tip_bias)] <- NA

#convert back to df
df <- as.data.frame(df3)

# Scaling and normalization are not needed https://github.com/dmlc/xgboost/issues/357
# NUMBER OF MARKERS
#log no_markers as long tail
#hist(df$no_markers)
#df$no_markers <- log(df$no_markers)
#hist(df$no_markers)

#NUMBER OF TIPS
#log tips as long tail
#hist(df$tips)
#df$tips <- log(df$tips)
#hist(df$tips)

#AGE OF TREE
#log age as long tail
#still strange
#hist(df$age)
#df$age <- log(df$age)
#hist(df$age)

#GLOBAL SAMPLING FRACTION
#hist(df$perc_sampling)
#arcsine
#df$perc_sampling <- asin(sqrt(df$perc_sampling))
#hist(df$perc_sampling)

#make sure factors are factors
df$order <- as.factor(df$order)
df$sse_model <- as.factor(df$sse_model)
df$year <- as.factor(df$year)

#convert characters to factors
as.factor(df)

df<-lapply(df,function(x){
  if(is.character(x)){x=as.factor(x)}else{x}
})

df<-as.data.frame(df)

str(df)

#remove study and model no (first two rows)
#samples per state varies among rows in models- should remove as well.
df <- df[, -which(colnames(df) %in% c('study',
                                      'model_no',
                                      'samples_per_state'))]

####
# Check correlation of continuous vars
####

df_cor <-
  df[, c(
    'tips',
    'no_markers',
    'age',
    'perc_sampling',
    'tip_bias'
  )]

cor(df_cor, use = "complete.obs")

#make data.table format
library(data.table)
df <- data.table(df, keep.rownames = FALSE)
head(df)
str(df)

#One-hot encoding
#Next step, we will transform the categorical data to dummy variables. This is the one-hot encoding step.
#The purpose is to transform each value of each categorical feature in a binary feature {0, 1}.
#Response is excluded because it will be our label column, the one we want to predict.

#allow NA values
options(na.action='na.pass')

#drop empty factor levels
#df<-droplevels(df)

#Formula div_inc~.-1 used above means transform all categorical features but column div_inc to binary values.
#The -1 is here to remove the first column which is full of 1 (this column is generated by the conversion).

#create sparse matrix
#keep reference levels for factors > 2 levels
#ensure that 1:8 are the factors
sparse_matrix <- sparse.model.matrix(div_inc ~ . -1 , data = df,verbose=T,
contrasts.arg = lapply(df[,1:8], contrasts, contrasts=FALSE ))

colnames(sparse_matrix)<-gsub("-","_",colnames(sparse_matrix))
str(sparse_matrix)
nrow(sparse_matrix)
colnames(sparse_matrix)

###
# Run an analysis
###

#set label
label <- as.numeric(df$div_inc)-1

#number of rows in entire matrix
n = nrow(sparse_matrix)

#training percentage
tp <- 0.8

#get indices of training rows
train.index = sample(n, floor(tp * n))
#subset training data
train.data = as.matrix(sparse_matrix[train.index,])
#labels (results) of training data
train.label = label[train.index]
#make DMatrix for train
dtrain <- xgb.DMatrix(data = train.data, label=train.label,missing=NA)

#get test data (whole dataset without training data)
test.data = as.matrix(sparse_matrix[-train.index,])
#labels (results) of test data
test.label = label[-train.index]
#make DMatrix for test
dtest <- xgb.DMatrix(data = test.data, label=test.label,missing=NA)

#run model
xgb.fit <-
  xgboost(
    booster = 'gbtree',
    data = dtrain,
    max.depth = 3, #maximum depth of a tree
    eta = 0.0001, #the learning rate: scale the contribution of each tree
    #gamma = 0, #minimum loss reduction required to make a further partition on a leaf node of the tree
    nthread = 1,
    nrounds = 20000, #max number of boosting iterations
    eval_metric = 'error',
    eval_metric = 'logloss',
    early_stopping_rounds = 20, #training with a validation set will stop if the performance doesn't improve for k rounds
    objective = "binary:logistic",
    print_every_n = 1000
  )

# Review the final model and results
xgb.fit

# Predict outcomes with the test data
xgb.probs <- as.data.frame(predict(xgb.fit, test.data))

#Turn prediction probabilities into classifications by thresholding at 0.5.
xgb.pred <- ifelse(xgb.probs > 0.5, 1, 0)

#Calculate the final accuracy
table(xgb.pred, test.label)

#prediction accuracy
mean(xgb.pred == test.label)

#confusion matrix
confusionMatrix (as.factor(xgb.pred), as.factor(test.label))

#store importances
imps <-
  xgb.importance(feature_names = sparse_matrix@Dimnames[[2]],
                 model = xgb.fit, )

#Gain
#the improvement in accuracy brought by a feature to the branches it is on.

#Cover
#measures the relative quantity of observations concerned by a feature.

#Frequency
#counts the number of times a feature is used in all generated trees.

#examine results
head(imps)

###
# Cross validation
###

#Using the inbuilt xgb.cv function, let's calculate the best nround for this model. In addition, this function also returns CV error, which is an estimate of test error.
xgbcv <-
  xgb.cv(
    data = dtrain,
    max.depth = 8,
    eta = 0.001,
    nthread = 1,
    nrounds = 20000,
    eval_metric = 'error',
    eval_metric = 'logloss',
    objective = "binary:logistic",
    print_every_n = 1000,
    early_stopping_rounds = 20,
    nfold = 5,
    showsd = T,
    stratified = T,
    maximize = F
  )

#cross validation error
print(xgbcv)

#####
# Parameter tuning
#####

#need to start with sparse matrix otherwise col names different between test and train sets
#convert to data frame for functions, fix names
sparse_matrix<-as.data.frame(as.matrix(sparse_matrix))
colnames(sparse_matrix)<-gsub("-","_",colnames(sparse_matrix))#seems redundant now
colnames(sparse_matrix)<-gsub(" ","_",colnames(sparse_matrix))#seems redundant now
colnames(sparse_matrix)

#check order is correct
sparse_matrix$perc_sampling==df$perc_sampling
sparse_matrix$age_inferred==df$age_inferred

#add div_inc column and rename
sparse_matrix<-cbind(sparse_matrix,df$div_inc)
colnames(sparse_matrix)[length(colnames(sparse_matrix))]<-"div_inc"
colnames(sparse_matrix)

#number of rows in entire matrix
n = nrow(sparse_matrix)

#training percentage
tp <- 0.8

#index
train.index = sample(n, floor(tp * n))
#subset training data
train.data = sparse_matrix[train.index,]
#get test data (whole dataset without training data)
test.data = sparse_matrix[-train.index,]

#no longer needed
#convert characters to factors
#fact_col <- colnames(train.data)[sapply(train.data,is.character)]
#for(i in fact_col) set(train.data,j=i,value = factor(train.data[[i]]))
#for(i in fact_col) set(test.data,j=i,value = factor(test.data[[i]]))

#create tasks
traintask <- makeClassifTask (data = train.data, target = "div_inc", positive = '1')
traintask
testtask <- makeClassifTask (data = test.data, target = "div_inc", positive = '1')

#already done
#do one hot encoding
#traintask <- createDummyFeatures (obj = traintask)
#testtask <- createDummyFeatures (obj = testtask)

#create learner
lrn <- makeLearner("classif.xgboost", predict.type = "response")
lrn$par.vals <-
  list(
    objective = "binary:logistic",
    eval_metric = "logloss",
    nrounds = 20000L,
    early_stopping_rounds = 20,
    print_every_n = 1000
  )

#set parameter space
params <-
  makeParamSet(
    makeIntegerParam("max_depth", lower = 2L, upper = 8L),
    makeNumericParam("min_child_weight", lower = 1L, upper = 10L),
    makeNumericParam("subsample", lower = 0.5, upper = 1),
    makeNumericParam("colsample_bytree", lower = 0.5, upper = 1),
    makeNumericParam("gamma", lower = 0, upper = 20),
    makeNumericParam("eta", lower = 0.0001, upper = 0.3)
  )

#set resampling strategy
rdesc <- makeResampleDesc("CV",
                          stratify = T, #ensure that distribution of target class is maintained in the resampled data sets.
                          iters = 20L)

#search strategy
ctrl <- makeTuneControlRandom(maxit = 40L) #random search to find the best parameters. maxit = number of models

#parameter tuning
mytune <-
  tuneParams(
    learner = lrn,
    task = traintask,
    resampling = rdesc,
    measures = acc,
    par.set = params,
    control = ctrl,
    show.info = T
  )
mytune

#set hyperparameters
lrn_tune <- setHyperPars(lrn,par.vals = mytune$x)

#train model
xgmodel <- train(learner = lrn_tune,task = traintask)

#predict model
xgpred <- predict(xgmodel,testtask)

#confusion matrix
confusionMatrix(xgpred$data$response,xgpred$data$truth)

###
# Resampling training and test datasets for cross validation
###

#remake sparse matrix
#allow NA values
#keep reference levels for factors > 2 levels
#ensure that 1:8 are the factors
sparse_matrix <- sparse.model.matrix(div_inc ~ . -1 , data = df,verbose=T,
                                     contrasts.arg = lapply(df[,1:8], contrasts, contrasts=FALSE ))

colnames(sparse_matrix)<-gsub("-","_",colnames(sparse_matrix))
str(sparse_matrix)
nrow(sparse_matrix)
colnames(sparse_matrix)

#set label
label <- as.numeric(df$div_inc)-1

#number of rows in entire matrix
n = nrow(sparse_matrix)

#training percentage
tp <- 0.8

#empty vector to store results
xg_dist <- vector()

#empty list to store importances
imps <- list()

#loop to run sampling 500 times
for (i in 1:500) {

  print(paste("Iteration :", i))

  #get indices of training rows
  train.index = sample(n, floor(tp * n))
  #subset training data
  train.data = as.matrix(sparse_matrix[train.index,])
  #labels (results) of training data
  train.label = label[train.index]
  #make DMatrix for train
  dtrain <- xgb.DMatrix(data = train.data, label=train.label, missing=NA)

  #get test data (whole dataset without training data)
  test.data = as.matrix(sparse_matrix[-train.index,])
  #labels (results) of test data
  test.label = label[-train.index]
  #make DMatrix for test
  dtest <- xgb.DMatrix(data = test.data, label=test.label, missing=NA)

  #run model
  xgb.fit <-
    xgboost(
      booster = 'gbtree',
      data = dtrain,
      max.depth = lrn_tune$par.vals$max_depth,
      eta = lrn_tune$par.vals$eta,
      gamma = lrn_tune$par.vals$gamma,
      min_child_weight = lrn_tune$par.vals$min_child_weight,
      subsample = lrn_tune$par.vals$subsample,
      colsample_bytree = lrn_tune$par.vals$colsample_bytree,
      nthread = 2,
      nrounds = 20000,
      eval_metric = 'error',
      eval_metric = 'logloss',
      early_stopping_rounds = 20,
      objective = "binary:logistic",
      print_every_n = 1000
    )

  # Review the model and results
  #xgb.fit

  # Predict outcomes with the test data
  xgb.probs <- as.data.frame(predict(xgb.fit, test.data))

  #Turn prediction probabilities into classifications by thresholding at 0.5.
  xgb.pred <- ifelse(xgb.probs > 0.5, 1, 0)

  #Calculate the final accuracy
  confusionMatrix (as.factor(xgb.pred), as.factor(test.label))

  #store prediction accuracy
  xg_dist[i] <- mean(xgb.pred == test.label)

  #store importances
  imps[[i]] <-
    xgb.importance(feature_names = sparse_matrix@Dimnames[[2]],
                   model = xgb.fit, )

}


#make data frame of accuracies
xg_dist <- data.frame(xg_dist)

str(xg_dist)

#histogram of accuracies
p <- ggplot(xg_dist, aes(x = xg_dist)) +
  geom_histogram(
    binwidth = 0.02,
    fill = "#69b3a2",
    color = "#e9ecef",
    alpha = 0.9
  ) +
  theme(plot.title = element_text(size = 15)) + xlab("xgboost accuracy") + ylab("count")

p

#mean value
mean(xg_dist$xg_dist)
min(xg_dist$xg_dist)
max(xg_dist$xg_dist)

#set theme
theme_set(theme_bw())

#uncomment to save histogram
ggsave("figures/hist_xgboost_accuracy.png")

#https://www.kaggle.com/rtatman/machine-learning-with-xgboost-in-r
#The top of the tree is on the left and the bottom of the tree is on the right. For features, the number next to it is "quality", which helps indicate how important this feature was across all the trees. Higher quality means a feature was more important. So we can tell that is_domestic was by far the most important feature across all of our trees, both because it's higher in the tree and also because it's quality score is very high.

#For the nodes with "Leaf", the number next to the "Leaf" is the average value the model returned across all trees if a a certain observation ended up in that leaf. Because we're using a logistic model here, it's telling us the log-odds rather than the probability. We can pretty easily convert the log odds to probability, though.
xgb.plot.multi.trees(feature_names = sparse_matrix@Dimnames[[2]],
                     model = xgb.fit)

#save figure
#ggsave("figures/xgboost_tree.png")

#save image
#save.image("outputs/xgboost_3.Rdata")

#load for replotting
#load("outputs/xgboost_2.Rdata")

options(na.action='na.omit')

