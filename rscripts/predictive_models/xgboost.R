
rm(list=ls())
library(xgboost)

#read in data
df <- read.csv("~/Downloads/sse_review_table - main_table.csv")

#check col names
colnames(df)

#choose the columns you want in model (plus study/model_no for formatting)
df<-df[,c('study','model_no','order','trait_type','sse_model','tips','year','no_markers','age','age_inferred',"div_inc")]

#make sure factors are factors
df$order <- as.factor(df$order)
df$trait_type <- as.factor(df$trait_type)
df$sse_model <- as.factor(df$sse_model)
df$year<-as.factor(df$year)

#reduce to a single binary result per model per study
library(dplyr)
df3 <- df %>%
  group_by(study, model_no) %>%
  dplyr::slice(which.max(div_inc))

#convert back to df
df<-as.data.frame(df3)

#remove study and model no (first two rows)
df<-df[,-1:-2]

library(data.table)
df <- data.table(df, keep.rownames = FALSE)
df[is.na(df)] <- 0
head(df)
str(df)

#can include ordinal vars
#ordinal variable : can take a limited number of values (like factor) ;
#these values are ordered (unlike factor). Here these ordered values are: Marked > Some > None
#GOOD FOR MUSSE?

#One-hot encoding
#Next step, we will transform the categorical data to dummy variables. This is the one-hot encoding step.

#The purpose is to transform each value of each categorical feature in a binary feature {0, 1}.
#Response is excluded because it will be our label column, the one we want to predict.
require(Matrix)
require(xgboost)
sparse_matrix <- sparse.model.matrix(div_inc~.-1, data = df)
head(sparse_matrix)

#Formula div_inc~.-1 used above means transform all categorical features but column div_inc to binary values. The -1 is here to remove the first column which is full of 1 (this column is generated by the conversion). For more information, you can type ?sparse.model.matrix in the console.

output_vector = df[,div_inc] == 1

#Build the model
bst <- xgboost(data = sparse_matrix, label = as.numeric(output_vector), max.depth = 4,
               eta = 1, nthread = 1, nrounds = 2,objective = "binary:logistic")

importance <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst)
head(importance)

#Gain is the improvement in accuracy brought by a feature to the branches it is on. The idea is that before adding a new split on a feature X to the branch there was some wrongly classified elements, after adding the split on this feature, there are two new branches, and each of these branch is more accurate (one branch saying if your observation is on this branch then it should be classified as 1, and the other branch saying the exact opposite).

#Cover measures the relative quantity of observations concerned by a feature.

#Frequency is a simpler way to measure the Gain. It just counts the number of times a feature is used in all generated trees. You should not use it (unless you know why you want to use it).

#We can go deeper in the analysis of the model. In the data.table above, we have discovered which features counts to predict if the illness will go or not. But we don’t yet know the role of these features. For instance, one of the question we may want to answer would be: does receiving a placebo treatment helps to recover from the illness?

#One simple solution is to count the co-occurrences of a feature and a class of the classification.

#For that purpose we will execute the same function as above but using two more parameters, data and label.

importanceRaw <- xgb.importance(feature_names = sparse_matrix@Dimnames[[2]], model = bst, data = sparse_matrix, label = output_vector)

importanceRaw

# Cleaning for better display
importanceClean <- importanceRaw[,`:=`(Cover=NULL, Frequency=NULL)]

head(importanceClean)

#First thing you notice is the new column Split. It is the split applied to the feature on a branch of one of the tree. Each split is present, therefore a feature can appear several times in this table. Here we can see the feature Age is used several times with different splits.

#How the split is applied to count the co-occurrences? It is always <. For instance, in the second line, we measure the number of persons under 61.5 years with the illness gone after the treatment.

#The two other new columns are RealCover and RealCover %. In the first column it measures the number of observations in the dataset where the split is respected and the label marked as 1. The second column is the percentage of the whole population that RealCover represents.

#Therefore, according to our findings, getting a placebo doesn’t seem to help but being younger than 61 years may help (seems logic).

#Plotting the feature importance
xgb.plot.importance(importance_matrix = importanceRaw)

#Let’s check some Chi2 between each of these features and the label.
#Higher Chi2 means better correlation.
c2 <- chisq.test(df$tips, output_vector)
print(c2)

#
c2 <- chisq.test(df$year, output_vector)
print(c2)
